{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9-Step Rhetorical Evaluation Notebook\n",
    "\n",
    "This notebook runs the \"From Evaluation to Growth\" analysis framework interactively.\n",
    "\n",
    "## The 9 Steps\n",
    "\n",
    "### Phase 1: EVALUATION\n",
    "1. **Critique** - Strengths and weaknesses assessment\n",
    "2. **Logic Check** - Internal consistency and argument flow\n",
    "3. **Logos Review** - Rational appeal (evidence, facts)\n",
    "4. **Pathos Review** - Emotional resonance\n",
    "5. **Ethos Review** - Credibility and authority markers\n",
    "\n",
    "### Phase 2: RISK\n",
    "6. **Blind Spots** - Overlooked areas and assumptions\n",
    "7. **Shatter Points** - Vulnerabilities and weak arguments\n",
    "\n",
    "### Phase 3: GROWTH\n",
    "8. **Bloom** - Emergent insights and connections\n",
    "9. **Evolve** - Synthesized improvement recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Add framework to path\n",
    "FRAMEWORK_ROOT = Path.cwd().parent.parent.parent\n",
    "sys.path.insert(0, str(FRAMEWORK_ROOT))\n",
    "\n",
    "# Import framework modules\n",
    "from framework.core import Atomizer, Corpus, AtomLevel\n",
    "from framework.analysis import EvaluationAnalysis\n",
    "\n",
    "print(f\"Framework root: {FRAMEWORK_ROOT}\")\n",
    "print(\"Evaluation module loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PROJECT_DIR = Path.cwd().parent\n",
    "PROJECT_NAME = PROJECT_DIR.name\n",
    "\n",
    "RAW_DIR = PROJECT_DIR / \"data\" / \"raw\"\n",
    "PROCESSED_DIR = PROJECT_DIR / \"data\" / \"processed\"\n",
    "CORPUS_FILE = RAW_DIR / f\"{PROJECT_NAME}_atomized.json\"\n",
    "\n",
    "print(f\"Project: {PROJECT_NAME}\")\n",
    "print(f\"Corpus: {CORPUS_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "if CORPUS_FILE.exists():\n",
    "    corpus = Atomizer.load_json(CORPUS_FILE)\n",
    "    print(f\"‚úÖ Loaded corpus: {corpus.name}\")\n",
    "    print(f\"   Themes: {corpus.count_atoms(AtomLevel.THEME)}\")\n",
    "    print(f\"   Sentences: {corpus.count_atoms(AtomLevel.SENTENCE)}\")\n",
    "else:\n",
    "    print(\"‚ùå Corpus not found. Run atomization first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Evaluation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation module instance\n",
    "evaluator = EvaluationAnalysis()\n",
    "print(f\"‚úÖ {evaluator.name}: {evaluator.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Configure LLM Provider\n",
    "\n",
    "For deeper insights on Critique, Bloom, and Evolve steps, you can configure an LLM provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Configuration (optional - set to None to skip)\n",
    "LLM_CONFIG = None  # Set to enable LLM-powered insights\n",
    "\n",
    "# Example configurations:\n",
    "# \n",
    "# Anthropic:\n",
    "# LLM_CONFIG = {\n",
    "#     \"provider\": \"anthropic\",\n",
    "#     \"model\": \"claude-sonnet-4-20250514\",\n",
    "#     \"api_key_env\": \"ANTHROPIC_API_KEY\"\n",
    "# }\n",
    "# \n",
    "# OpenAI:\n",
    "# LLM_CONFIG = {\n",
    "#     \"provider\": \"openai\",\n",
    "#     \"model\": \"gpt-4o\",\n",
    "#     \"api_key_env\": \"OPENAI_API_KEY\"\n",
    "# }\n",
    "# \n",
    "# Ollama (local):\n",
    "# LLM_CONFIG = {\n",
    "#     \"provider\": \"ollama\",\n",
    "#     \"model\": \"llama3\"\n",
    "# }\n",
    "\n",
    "if LLM_CONFIG:\n",
    "    print(f\"LLM configured: {LLM_CONFIG['provider']} / {LLM_CONFIG['model']}\")\n",
    "else:\n",
    "    print(\"LLM not configured - running heuristic analysis only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure which steps to run (1-9, or subset)\n",
    "STEPS_TO_RUN = [1, 2, 3, 4, 5, 6, 7, 8, 9]  # All steps\n",
    "\n",
    "# Run analysis\n",
    "config = {\n",
    "    \"steps\": STEPS_TO_RUN,\n",
    "}\n",
    "if LLM_CONFIG:\n",
    "    config[\"llm\"] = LLM_CONFIG\n",
    "\n",
    "print(\"Running evaluation analysis...\")\n",
    "print(f\"Steps: {STEPS_TO_RUN}\\n\")\n",
    "\n",
    "result = evaluator.analyze(corpus, config=config)\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Overall Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary\n",
    "summary = result.data.get(\"summary\", {})\n",
    "overall_score = summary.get(\"overall_score\", 0)\n",
    "phase_scores = summary.get(\"phase_scores\", {})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Score bar visualization\n",
    "def score_bar(score, width=30):\n",
    "    filled = int(score / 100 * width)\n",
    "    return \"‚ñà\" * filled + \"‚ñë\" * (width - filled)\n",
    "\n",
    "print(f\"\\nOverall Score: {overall_score:.1f}/100\")\n",
    "print(f\"[{score_bar(overall_score)}]\")\n",
    "\n",
    "print(\"\\nPhase Scores:\")\n",
    "for phase, score in phase_scores.items():\n",
    "    emoji = {\"evaluation\": \"üîç\", \"risk\": \"‚ö†Ô∏è\", \"growth\": \"üå±\"}.get(phase, \"üìå\")\n",
    "    print(f\"  {emoji} {phase.title():12} {score:5.1f} [{score_bar(score, 20)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Step Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display each step result\n",
    "phases = result.data.get(\"phases\", {})\n",
    "\n",
    "STEP_ICONS = {\n",
    "    \"critique\": \"üîç\",\n",
    "    \"logic_check\": \"üß†\",\n",
    "    \"logos\": \"üìä\",\n",
    "    \"pathos\": \"üíì\",\n",
    "    \"ethos\": \"üë§\",\n",
    "    \"blind_spots\": \"üëÅÔ∏è\",\n",
    "    \"shatter_points\": \"üí•\",\n",
    "    \"bloom\": \"üå∏\",\n",
    "    \"evolve\": \"üöÄ\",\n",
    "}\n",
    "\n",
    "for phase_name, phase_data in phases.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Phase: {phase_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for step_name, step_data in phase_data.items():\n",
    "        icon = STEP_ICONS.get(step_name, \"üìå\")\n",
    "        score = step_data.get(\"score\", 0)\n",
    "        step_num = step_data.get(\"step_number\", 0)\n",
    "        \n",
    "        print(f\"\\n{icon} Step {step_num}: {step_name.replace('_', ' ').title()}\")\n",
    "        print(f\"   Score: {score:.1f}/100 [{score_bar(score, 15)}]\")\n",
    "        \n",
    "        # Show key metrics\n",
    "        metrics = step_data.get(\"metrics\", {})\n",
    "        if metrics:\n",
    "            print(\"   Metrics:\")\n",
    "            for k, v in list(metrics.items())[:4]:\n",
    "                print(f\"      ‚Ä¢ {k.replace('_', ' ').title()}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all findings\n",
    "all_findings = []\n",
    "\n",
    "for phase_name, phase_data in phases.items():\n",
    "    for step_name, step_data in phase_data.items():\n",
    "        for finding in step_data.get(\"findings\", []):\n",
    "            finding[\"step\"] = step_name\n",
    "            all_findings.append(finding)\n",
    "\n",
    "# Group by type\n",
    "TYPE_EMOJI = {\n",
    "    \"strength\": \"‚úÖ\",\n",
    "    \"weakness\": \"‚ö†Ô∏è\",\n",
    "    \"blind_spot\": \"üëÅÔ∏è\",\n",
    "    \"shatter_point\": \"üí•\",\n",
    "    \"insight\": \"üí°\",\n",
    "    \"observation\": \"üìù\",\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîé KEY FINDINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show strengths\n",
    "strengths = [f for f in all_findings if f.get(\"type\") == \"strength\"]\n",
    "if strengths:\n",
    "    print(\"\\n‚úÖ Strengths:\")\n",
    "    for f in strengths:\n",
    "        print(f\"   ‚Ä¢ [{f['step']}] {f.get('description', '')}\")\n",
    "\n",
    "# Show weaknesses\n",
    "weaknesses = [f for f in all_findings if f.get(\"type\") == \"weakness\"]\n",
    "if weaknesses:\n",
    "    print(\"\\n‚ö†Ô∏è Weaknesses:\")\n",
    "    for f in weaknesses:\n",
    "        print(f\"   ‚Ä¢ [{f['step']}] {f.get('description', '')}\")\n",
    "\n",
    "# Show insights\n",
    "insights = [f for f in all_findings if f.get(\"type\") == \"insight\"]\n",
    "if insights:\n",
    "    print(\"\\nüí° Insights:\")\n",
    "    for f in insights:\n",
    "        print(f\"   ‚Ä¢ [{f['step']}] {f.get('description', '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top recommendations\n",
    "top_recs = summary.get(\"top_recommendations\", [])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã TOP RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, rec in enumerate(top_recs[:10], 1):\n",
    "    print(f\"\\n{i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to processed directory\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "output_file = PROCESSED_DIR / \"evaluation_data.json\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result.to_dict(), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Individual Steps\n",
    "\n",
    "You can also run individual steps for deeper analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only specific steps\n",
    "# Modify STEPS_TO_RUN to run a subset\n",
    "\n",
    "# Example: Run only Logos, Pathos, Ethos (the rhetorical appeals)\n",
    "APPEALS_ONLY = [3, 4, 5]\n",
    "\n",
    "appeals_result = evaluator.analyze(corpus, config={\"steps\": APPEALS_ONLY})\n",
    "\n",
    "print(\"\\nüìä Rhetorical Appeals Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "appeals_phases = appeals_result.data.get(\"phases\", {})\n",
    "eval_phase = appeals_phases.get(\"evaluation\", {})\n",
    "\n",
    "for step_name in [\"logos\", \"pathos\", \"ethos\"]:\n",
    "    if step_name in eval_phase:\n",
    "        score = eval_phase[step_name].get(\"score\", 0)\n",
    "        icon = STEP_ICONS.get(step_name, \"üìå\")\n",
    "        print(f\"{icon} {step_name.title():8} {score:5.1f} [{score_bar(score, 20)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Visualize results**: Open `visualization.ipynb` or run:\n",
    "   ```bash\n",
    "   lingframe visualize -p <project-name>\n",
    "   ```\n",
    "\n",
    "2. **Export report**: The JSON results can be used to generate custom reports\n",
    "\n",
    "3. **Iterate**: Make improvements based on recommendations and re-run analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
